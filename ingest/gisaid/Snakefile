
import os
configfile: os.path.join(workflow.basedir, "config.yaml")

include: "../../shared/vendored/snakemake/remote_files.smk"
include: "../rules/genoflu.smk"
include: "../rules/upload_to_s3.smk"


# The Genoflu workflow will create "gisaid/results/metadata.tsv" with GenoFLU information
# So make that the default workflow target. This will force provisioning of upstream
# metadata & sequences
rule all:
    input:
        metadata="gisaid/results/metadata.tsv",

rule upload_all:
    input:
        metadata="gisaid/s3/metadata.done",
        sequences=expand("gisaid/s3/sequences_{segment}.done", segment=config["segments"]),

rule get_sequence:
    """
    Provisions the curated sequences (ultimately from the seasonal-flu ingest)
    into the location where both the GenoFlu workflow and the upload rules can access them.
    (Note: We could use a different location and skip `provision_genoflu_sequences` but
    we want to upload the sequences at the end of the workflow in order to keep metadata
    & sequences in-sync.)
    """
    input:
        path_or_url(config['sequences'])
    output:
        "gisaid/results/sequences_{segment}.fasta"
    shell:
        """
        if [[ {input[0]} == *.xz ]]; then
            xz -dc {input[0]} > {output[0]}
        elif [[ {input[0]} == *.zst ]]; then
            zstd -dc {input[0]} > {output[0]}
        else
            cp {input[0]} {output[0]}
        fi
        """

rule get_metadata:
    """
    Provisions the metadata in the location the genoflu workflow expects it.
    """
    input:
        path_or_url(config['metadata'])
    output:
        "gisaid/data/metadata_combined.tsv"
    shell:
        """
        cp {input[0]} {output[0]}
        """
